experiment:
    name: clamp_no_conv
    project_name: VitVQGAN
    output_folder: outputs/vitvqgan
    max_train_examples: 1000000
    save_every: 500
    eval_every: 500
    sample_every: 500
    log_every: 100
    log_level: info
    resume_path_from_checkpoint: null


codebook:
        codebook_dim: 32
        beta : 0.25
        codebook_size: 8192

model:
    name: vitvqgan
    transformer:
        dim : 512
        patch_size : 8
        n_heads : 8
        d_head : 64
        depth : 6
        dropout : 0.
        mlp_dim: 2048


dataset:
    name: coco
    params:
        train_path:  /home/pranoy/datasets/coco2017
        val_path: null
        num_workers: 4
        pin_memory: True
        batch_size: 8
        persistent_workers: True
        shuffle : True
        train_test_split : 0.9
    preprocessing:
        resolution: 256
        center_crop: False
        random_flip: True
        random_crop: True
        mean : null
        std : null
        scale : 0.66

optimizer:
    name: adamw
    params: 
        learning_rate: 0.0001
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.0001
        epsilon: 1e-8

lr_scheduler:
    scheduler: cosine_with_warmup
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 50000
        decay_steps: 100000

losses : 
    per_loss_weight : 1
    adv_loss_weight :  0.1
    logit_laplace_weight :  1

training:
    gradient_accumulation_steps: 2
    mixed_precision: "bf16"
    seed: 42
    num_epochs: 200
    max_grad_norm: 1.0


